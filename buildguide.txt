Below is a deep-dive project description that outlines how you can transition the existing XenArch toolset into a functional web application. The goal of this app is to provide a user-friendly front-end for uploading GeoTIFF files, configuring parameters, running the analysis pipeline, and then displaying the top N terrain grids of interest.

---

## High-Level Architecture

A typical approach for turning a command-line Python tool into a web application involves several layers:

1. **Frontend**  
   - A web-based user interface that allows file upload and configuration of parameters.
   - Displays results, such as top N grids or summary metrics.

2. **Backend**  
   - A server (Flask, FastAPI, or Django in Python) that receives the uploaded file, stores it, and executes the XenArch processing pipeline.
   - Manages asynchronous or synchronous job execution (depending on resource constraints and user requirements).
   - Exposes endpoints or routes for retrieving analysis results (images, JSON data, etc.).

3. **Database / Storage** (Optional in early phases)  
   - Stores metadata and analysis results for repeatability or for showing historical runs.
   - A simple file-based approach can work initially (storing results in a local folder structure).

4. **Infrastructure** (Optional or minimal in early phases)  
   - Docker containers or a VM-based deployment.
   - Logging, concurrency, and scheduling considerations.

Below is an outline of the user flow, followed by a proposed technical solution.

---

## User Flow

1. **Upload GeoTIFF**  
   On the main page, the user selects a local `.tif` file and uploads it via a file input form.

2. **Configure Parameters**  
   The user sets the parameters that XenArch requires:  
   - Grid Size (e.g., 512)  
   - Overlap (e.g., 64)  
   - Fractal Dimension Range (fd_min, fd_max)  
   - R-squared Minimum (r2_min)  
   - Max Samples to Display (max_samples)  
   - CPU Fraction (cpu_fraction)

3. **Submit for Processing**  
   When the user clicks “Start Analysis,” the server receives the file and parameters, triggers the XenArch pipeline, and monitors progress.

4. **View Results**  
   - The server reads the generated JSON metrics and sorts grids by some combination of fractal dimension and R² value.  
   - The top N results (based on the user’s chosen constraints) are displayed as thumbnail images in a grid or list view on the results page.  
   - The user can click on a thumbnail to view details (elevation stats, fractal dimension, R², position, etc.).

---

## Technical Plan

### 1. Frontend

A minimal viable front end can be built with:

- **HTML/CSS/JS** (Vanilla, React, Vue, etc.):  
  - A page with a file input (`<input type="file" />`) for the GeoTIFF.  
  - Input fields (or sliders) for `grid_size`, `overlap`, `fd_min`, `fd_max`, `r2_min`, `max_samples`, `cpu_fraction`.  
  - A “Submit” button that sends the data via a POST request to the backend.

- **UI Layout**  
  - **Index Page**: Upload form + parameter configuration.  
  - **Progress / Results Page**: Display “Analysis in Progress” or show results once available.  

You can optionally add JavaScript to poll the server for job status if asynchronous. For a simpler synchronous approach, the user waits for the request to finish and then sees the results.

### 2. Backend (Flask Example)

Below is a conceptual snippet of how the Flask backend might look. The same concepts apply to FastAPI or Django, just with different routing syntax.

```python
# app.py (Flask example)
from flask import Flask, request, render_template, send_from_directory
from werkzeug.utils import secure_filename
import os
import subprocess
import uuid
import shutil
import json
from pathlib import Path

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = './uploads'
app.config['RESULTS_FOLDER'] = './analysis_results'

@app.route('/')
def index():
    return render_template('index.html')  # Your front page with file upload and parameter form

@app.route('/upload', methods=['POST'])
def upload():
    # 1. Get the uploaded file
    f = request.files['terrain_file']  # name from the form input
    filename = secure_filename(f.filename)
    
    # 2. Save file to an uploads directory
    upload_id = str(uuid.uuid4())  # unique ID to track user job
    job_folder = os.path.join(app.config['UPLOAD_FOLDER'], upload_id)
    os.makedirs(job_folder, exist_ok=True)
    
    saved_path = os.path.join(job_folder, filename)
    f.save(saved_path)

    # 3. Get parameters from the form
    grid_size = request.form.get('grid_size', '512')
    overlap = request.form.get('overlap', '64')
    fd_min = request.form.get('fd_min', '0.0')
    fd_max = request.form.get('fd_max', '0.8')
    r2_min = request.form.get('r2_min', '0.8')
    max_samples = request.form.get('max_samples', '16')
    cpu_fraction = request.form.get('cpu_fraction', '0.8')
    
    # 4. Create an output folder
    output_dir = os.path.join(app.config['RESULTS_FOLDER'], upload_id)
    os.makedirs(output_dir, exist_ok=True)
    
    # 5. Call the XenArch pipeline (synchronously or asynchronously)
    # For example, calling main.py complete:
    command = [
        'python', 'main.py', 'complete',
        '-i', saved_path,
        '-o', output_dir,
        '--grid-size', grid_size,
        '--overlap', overlap,
        '--fd-min', fd_min,
        '--fd-max', fd_max,
        '--r2-min', r2_min,
        '--max-samples', max_samples,
        '--cpu-fraction', cpu_fraction,
        '--plot-output', output_dir,  # to store the plots in the same folder
        '-v'
    ]
    
    subprocess.run(command, check=True)

    # 6. After processing, we can read top grids from JSON files or rely on the 'analyze_results' step
    # The 'analyze' step creates a 'filtered_samples.png' etc. We'll parse the JSON and pick the top N
    # for demonstration. But if the pipeline already generates the final images, we just serve them.

    # 7. Redirect or display results
    return f"Analysis complete. Access results for job: {upload_id}"

@app.route('/results/<job_id>')
def get_results(job_id):
    # 1. The output directory for this job
    output_dir = os.path.join(app.config['RESULTS_FOLDER'], job_id)

    # 2. Gather the top N JSON or TIF files
    # (Optionally, parse an aggregator file or rely on the JSON files.)
    
    # Example: find all JSON files, sort by fractal dimension or some metric
    json_paths = list(Path(output_dir).glob("*.json"))
    metrics = []
    for path in json_paths:
        with open(path) as f:
            data = json.load(f)
            metrics.append(data)
    
    # Sort by fractal_dimension descending
    metrics.sort(key=lambda x: x['metrics']['fractal_dimension'], reverse=True)
    
    # Take the top N
    top_n = metrics[:10]  # or user-defined number
    
    # 3. Build an HTML snippet or pass this to a template
    # Each item has a .tif path -> We can generate a small PNG preview or embed 
    # or route a custom /serve_tif/ endpoint to serve an inline image (converted to PNG).
    # For simplicity, we'll just list them out.

    result_html = "<h1>Top 10 Grids by Fractal Dimension</h1><ul>"
    for i, m in enumerate(top_n):
        grid_id = m['grid_id']
        fractal_dim = m['metrics']['fractal_dimension']
        r2 = m['metrics']['r_squared']
        tiff_name = grid_id + ".tif"
        result_html += f"<li>{i+1}. <strong>{grid_id}</strong> FD={fractal_dim:.3f}, R2={r2:.3f} - TIF: {tiff_name}</li>"
    result_html += "</ul>"

    return result_html

if __name__ == '__main__':
    app.run(debug=True)
```

**Key Points in This Example**  
- We generate a unique `job_id` for each upload, then store inputs and outputs in separate folders.  
- We make a call to `subprocess.run` to invoke your existing pipeline from `main.py`. This is the simplest approach to reuse your existing CLI interface.  
- We store all JSON results in the `output_dir`, and can parse them afterward to pick out top candidates.

### 3. Generating Thumbnails

To display the top N grids of interest, we want to embed images on the frontend. GeoTIFF files are not directly displayable in a browser in their raw form, so you typically create PNG or JPEG thumbnails. Here are a few ways:

1. **On-the-Fly Conversion**  
   Write a Flask route like `/thumbnail/<job_id>/<grid_id>` that reads the `.tif`, uses `rasterio` or `PIL` to convert it to a PNG in memory, then returns an image response (`Content-Type: image/png`).

2. **Pre-Generate Thumbnails**  
   Whenever a grid is split, also generate a small PNG version and store it alongside the `.tif` (for example, `grid_00000_00000_thumb.png`). Then the frontend can load those PNGs directly.

A quick snippet for on-the-fly conversion might look like:

```python
@app.route('/thumbnail/<job_id>/<grid_id>')
def serve_thumbnail(job_id, grid_id):
    output_dir = os.path.join(app.config['RESULTS_FOLDER'], job_id)
    tiff_path = os.path.join(output_dir, f"{grid_id}.tif")
    
    if not os.path.exists(tiff_path):
        return "TIF Not Found", 404

    # Convert TIF to PNG in memory
    from io import BytesIO
    import rasterio
    from PIL import Image
    import numpy as np

    with rasterio.open(tiff_path) as src:
        data = src.read(1)
        # Convert to a valid display range
        data_min, data_max = np.nanpercentile(data, [2, 98])
        data_clipped = np.clip(data, data_min, data_max)
        # Normalize 0-255
        data_norm = ((data_clipped - data_min) / (data_max - data_min)) * 255.0
        data_norm = data_norm.astype(np.uint8)

        # Create PIL image in grayscale
        img = Image.fromarray(data_norm, mode='L')

    # Save to in-memory buffer
    buf = BytesIO()
    img.save(buf, format='PNG')
    buf.seek(0)

    return send_from_directory(directory=output_dir, filename=None, mimetype='image/png',
                               as_attachment=False, attachment_filename=f"{grid_id}.png",
                               file=buf)
```

(This snippet combines conceptually how you’d generate a PNG in memory. Actual code might differ slightly.)

### 4. Sorting and Filtering (Top N)

You can rely on the JSON metrics generated by `metrics_parser` or from the final “analyze” step to determine which grids are interesting. The typical flow is:

1. Read all JSON files (one per grid).  
2. Filter out those that don’t meet `fd_min`, `fd_max`, and `r2_min`.  
3. Sort by fractal dimension or any other metric.  
4. Present the top N in a gallery grid in your HTML page.

If you want the user to dynamically set `fd_min` or `r2_min` on the results page (rather than prior to the pipeline run), you can avoid re-running the pipeline by just re-filtering in Python or JavaScript. The user can then get near-instant feedback. This approach is feasible if the number of grids (and their JSON metrics) is not too large.

---

## Deployment Considerations

- **Local/Single-User Scenario**: If you only need local usage, you can run the Flask app on your machine, place it behind localhost, and manually open the UI in a browser.
- **Server Deployment**: Containerize the application with Docker, ensuring the environment has all required libraries (rasterio, GDAL, etc.). A typical Dockerfile might start from `tiangolo/uwsgi-nginx-flask` or a `python:3.x-slim` base with GDAL installed.

```dockerfile
FROM python:3.9-slim

# Install dependencies (incl. GDAL for rasterio)
RUN apt-get update && apt-get install -y gdal-bin libgdal-dev

# Create working directory
WORKDIR /app

# Copy your requirements and install
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# Copy project
COPY . .

# Expose port and start the app
EXPOSE 5000
CMD ["python", "app.py"]
```

- **Asynchronous / Long-Running Jobs**: If you anticipate very large TIFFs or many concurrent users, consider a job queue like Celery or RQ (Redis Queue). This way, the main Flask thread is not blocked by the analysis. Users can start a job and return later to retrieve results.

---

## Summary of Steps

1. **Set Up a Python Web Server** (Flask, FastAPI, or Django).  
2. **Create a Route** to handle file uploads and parameter retrieval.  
3. **Save the Uploaded TIFF** to a dedicated folder.  
4. **Run the XenArch Pipeline** using either:  
   - Subprocess calls to `main.py` (quick method).  
   - Direct function calls (more modular, but requires refactoring the command-line structure).  
5. **Store the Results** (JSON, TIF slices, etc.) in a known location.  
6. **Create a Results Route** that:  
   - Reads the JSON files.  
   - Filters/sorts them based on fractal dimension, R², or other metrics.  
   - Presents the top N in an HTML or JSON response.  
7. **(Optional) Serve Thumbnails** for each grid, either pre-generated or on demand.  
8. **Frontend**  
   - Build a simple form that points to `/upload` with the required parameters.  
   - Provide a results page or link to `/results/<job_id>` that lists or displays images.  

With these changes, you’ll have a fully functioning web app that turns XenArch’s CLI pipeline into an interactive, user-friendly environment. 

---

## Extended Ideas

- **Automatic 2D/3D Visualization**: You could integrate something like `ipyleaflet` or `CesiumJS` for advanced 3D terrain visualization if you want more interactive experiences. This is more complex but very powerful for geospatial data. 
- **Database Integration**: Store job metadata (timestamp, filename, user, top metrics) in a database (e.g., SQLite/Postgres). This makes it easier to manage multiple analyses or re-run them. 
- **User Authentication**: If the application is public or multi-user, incorporate a login system to separate user workspaces. 
- **Queue/Task Scheduling**: For large-scale usage or heavy computations, use Celery/RQ for asynchronous tasks. The user can see job status (“in progress,” “completed,” “failed”) and retrieve results when ready.

---

### Conclusion

This plan provides a blueprint for transforming XenArch’s existing tools into a web-based application. By leveraging a Python web framework (Flask shown here), you can quickly build a frontend for uploading GeoTIFF files, configuring the fractal analysis, and visualizing the most interesting slices. Over time, you can enrich this basic workflow with more sophisticated UI components, asynchronous processing, or advanced data management.
